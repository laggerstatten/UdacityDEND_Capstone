{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import types as t\n",
    "\n",
    "from sql_queries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def parsell(string):\n",
    "    string = string.strip().lower()\n",
    "\n",
    "    if string.endswith('w') or string.endswith('s'):\n",
    "        sign = -1\n",
    "    else:\n",
    "        sign = 1\n",
    "\n",
    "    string = re.sub(r\"[^0-9.]\", \" \", string).strip()\n",
    "\n",
    "    numeric_ll = float(string)\n",
    "    return numeric_ll * sign\n",
    "\n",
    "def parquet_wr(parquet_filename, df):\n",
    "\n",
    "    start_time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "    \n",
    "    # Write dataframe to parquet file:\n",
    "    df.write.mode(\"overwrite\").parquet(parquet_filename + \"_\" + start_time)\n",
    "\n",
    "    # Read parquet file to Spark dataframe:\n",
    "    df = spark.read.parquet(parquet_filename + \"_\" + start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START ETL pipeline process\n"
     ]
    }
   ],
   "source": [
    "# Start the clocks\n",
    "#start = datetime.now()\n",
    "#start_str = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "print(\"START ETL pipeline process\")\n",
    "results_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def define_paths():\n",
    "    \"\"\"\n",
    "    Define data locations.\n",
    "    \"\"\"\n",
    "    path_d = {}\n",
    "\n",
    "    path_d[\"input_data\"] = 'data/'\n",
    "    path_d[\"output_data\"] = 'data/output_data/'      \n",
    "    path_d[\"data_storage\"] = 'parquet' \n",
    "    path_d[\"hurdat\"] = 'https://www.aoml.noaa.gov/hrd/hurdat/hurdat2.html'\n",
    "        \n",
    "    return path_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "path_d = define_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Create Spark session.\n",
    "    \"\"\"\n",
    "    print(\"Create Spark session\")\n",
    "    spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Spark session\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-64ade42de1c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create Spark session for the pipeline.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_spark_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-b21a3d2eaa14>\u001b[0m in \u001b[0;36mcreate_spark_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Create Spark session\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                      \u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.jars.packages\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"org.apache.hadoop:hadoop-aws:2.7.0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m                      \u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                         \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    126\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "# Create Spark session for the pipeline.\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef parse_input_files(path_d, path, extension):\\n    # Get (from directory) the files matching extension\\n    all_files = []\\n    for root, dirs, files in os.walk(path):\\n        files = glob.glob(os.path.join(root, extension))\\n        for f in files :\\n            all_files.append(os.path.abspath(f))\\n\\n    return all_files\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def parse_input_files(path_d, path, extension):\n",
    "    # Get (from directory) the files matching extension\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        files = glob.glob(os.path.join(root, extension))\n",
    "        for f in files :\n",
    "            all_files.append(os.path.abspath(f))\n",
    "\n",
    "    return all_files\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_files = parse_input_files(PATHS, PATHS[\"i94_data\"], \"*.sas7bdat\")\\n    input_files_reordered = reorder_paths(input_files)\\n    PATHS[\"i94_files\"] = input_files_reordered\\n    print(f\"i94_files: {PATHS[\\'i94_files\\']}\")\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse input data dir\n",
    "\"\"\"\n",
    "input_files = parse_input_files(PATHS, PATHS[\"i94_data\"], \"*.sas7bdat\")\n",
    "    input_files_reordered = reorder_paths(input_files)\n",
    "    PATHS[\"i94_files\"] = input_files_reordered\n",
    "    print(f\"i94_files: {PATHS['i94_files']}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_hurdat_data(spark, path_d):\n",
    "    \"\"\"\n",
    "    Load input data (HURDAT) from input path\n",
    "    Write / read the data to / from Spark\n",
    "    Store the data as parquet staging files\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Processing HURDAT data\")\n",
    "    \n",
    "    # Read CSV\n",
    "    col_names = ['Date','Time','RecordIdentifier','SystemStatus','Latitude','Longitude','MaxSustWind','MaxPressure',\n",
    "                 'NE34','SE34','SW34','NW34',\n",
    "                 'NE50','SE50','SW50','NW50',\n",
    "                 'NE64','SE64','SW64','NW64']\n",
    "    hurdat_df = pd.read_csv(path_d[\"hurdat\"], skiprows = 2, low_memory=False, names=col_names)\n",
    "    \n",
    "    # Initial cleaning / reshaping\n",
    "    \n",
    "    #remove ghost row\n",
    "    hurdat_df = hurdat_df.drop([0])\n",
    "\n",
    "    #check if row is convoluted header row (contains ALPHA characters)\n",
    "    hurdat_df['IsStormHdr'] = ~hurdat_df['Date'].str.isdigit()\n",
    "\n",
    "    #create empty columns to receive header data\n",
    "    hurdat_df['StormIdentifier'] = ''\n",
    "    hurdat_df['StormName'] = ''\n",
    "    hurdat_df['StormSamples'] = ''\n",
    "    \n",
    "    #Iterate over rows to get header data and write to list\n",
    "    Lidentifier = []\n",
    "    Lname = []\n",
    "    Lsamples = []\n",
    "\n",
    "    identifier = \"\"\n",
    "    name = \"\"\n",
    "    samples = \"\"\n",
    "\n",
    "    for row in hurdat_df.itertuples(index=True):\n",
    "        if (getattr(row, \"IsStormHdr\") == True):\n",
    "            identifier = getattr(row, \"Date\")\n",
    "            name = getattr(row, \"Time\")\n",
    "            samples = getattr(row, \"RecordIdentifier\")\n",
    "        Lidentifier.append(identifier)\n",
    "        Lname.append(name)\n",
    "        Lsamples.append(samples)    \n",
    "    \n",
    "    #write list data into dataframe\n",
    "    hurdat_df.StormIdentifier = Lidentifier\n",
    "    hurdat_df.StormName = Lname\n",
    "    hurdat_df.StormSamples = Lsamples    \n",
    "    \n",
    "    #split into storms and tracks\n",
    "    hurdat_storms_df = hurdat_df[hurdat_df['IsStormHdr'] == True].copy()\n",
    "    hurdat_storms_df = hurdat_storms_df[['StormIdentifier','StormName','StormSamples']]\n",
    "\n",
    "    hurdat_tracks_df = hurdat_df[hurdat_df['IsStormHdr'] == False].copy()  \n",
    "    \n",
    "    \n",
    "    Tlatitude = [parsell(lat) for lat in hurdat_tracks_df['Latitude']]\n",
    "    hurdat_tracks_df['Latitude'] = Tlatitude\n",
    "\n",
    "    Tlongitude = [parsell(lon) for lon in hurdat_tracks_df['Longitude']]\n",
    "    hurdat_tracks_df['Longitude'] = Tlongitude \n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # Read data to Spark\n",
    "    print(\"Reading HURDAT to Spark\")\n",
    "    hurdat_storm_schema = t.StructType([\n",
    "                                t.StructField(\"StormIdentifier\", t.StringType(), False),\n",
    "                                t.StructField(\"StormName\", t.StringType(), False),\n",
    "                                t.StructField(\"StormSamples\", t.StringType(), False),\n",
    "                            ])\n",
    "\n",
    "    hurdat_track_schema = t.StructType([\n",
    "                                t.StructField('Date', t.StringType(), False),\n",
    "                                t.StructField('Time', t.StringType(), False),\n",
    "                                t.StructField('RecordIdentifier', t.StringType(), False),\n",
    "                                t.StructField('SystemStatus', t.StringType(), False),\n",
    "                                t.StructField('Latitude', t.StringType(), False),\n",
    "                                t.StructField('Longitude', t.StringType(), False),\n",
    "                                t.StructField('MaxSustWind', t.StringType(), False),\n",
    "                                t.StructField('MaxPressure', t.StringType(), False),                   \n",
    "                                t.StructField('NE34', t.StringType(), False),\n",
    "                                t.StructField('SE34', t.StringType(), False),\n",
    "                                t.StructField('SW34', t.StringType(), False),\n",
    "                                t.StructField('NW34', t.StringType(), False),    \n",
    "                                t.StructField('NE50', t.StringType(), False),\n",
    "                                t.StructField('SE50', t.StringType(), False),\n",
    "                                t.StructField('SW50', t.StringType(), False),\n",
    "                                t.StructField('NW50', t.StringType(), False),\n",
    "                                t.StructField('NE64', t.StringType(), False),\n",
    "                                t.StructField('SE64', t.StringType(), False),\n",
    "                                t.StructField('SW64', t.StringType(), False),\n",
    "                                t.StructField('NW64', t.StringType(), False),\n",
    "                                t.StructField('IsStormHdr', t.StringType(), False),             \n",
    "                                t.StructField(\"Identifier\", t.StringType(), False),\n",
    "                                t.StructField(\"Name\", t.StringType(), False),\n",
    "                                t.StructField(\"Samples\", t.StringType(), False)\n",
    "                                ])\n",
    "\n",
    "    hurdat_storms_df_spark = spark.createDataFrame(hurdat_storms_df, schema=hurdat_storm_schema)\n",
    "    hurdat_tracks_df_spark = spark.createDataFrame(hurdat_tracks_df, schema=hurdat_track_schema)\n",
    "\n",
    "    \n",
    "    parquet_wr(path_d[\"output_data\"] + \"hurdat_storms_stage.parquet\", hurdat_storms_df_spark)\n",
    "    parquet_wr(path_d[\"output_data\"] + \"hurdat_tracks_stage.parquet\", hurdat_tracks_df_spark)\n",
    "\n",
    "    \n",
    "    print(\"HURDAT processing complete\")\n",
    "\n",
    "    return hurdat_storms_df_spark, hurdat_tracks_df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing HURDAT data\n",
      "Reading HURDAT to Spark\n",
      "HURDAT processing DONE\n"
     ]
    }
   ],
   "source": [
    "# Process all input files\n",
    "hurdat_df_spark = process_hurdat_data(spark, path_d)\n",
    "hurdat_storms_df_spark = hurdat_df_spark[0]\n",
    "hurdat_tracks_df_spark = hurdat_df_spark[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Cleaning the data:\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Cleaning the data:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Process Dimension tables.\n",
    "\n",
    "def process_joined_hurdat_data( spark, path_d, hurdat_storms_df_spark, hurdat_tracks_df_spark):\n",
    "    \"\"\"\n",
    "    Load input data\n",
    "    Join tables\n",
    "    Write / read the data to / from Spark\n",
    "    Store the data as parquet dimension files\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Creating table\")\n",
    "\n",
    "    hurdat_df_spark_joined = hurdat_storms_df_spark\\\n",
    "                        .join(hurdat_tracks_df_spark, \\\n",
    "                        (hurdat_storms_df_spark.StormIdentifier == hurdat_tracks_df_spark.Identifier))   \n",
    "    \n",
    "    # Create table\n",
    "    hurdat_df_spark_joined.createOrReplaceTempView(\"hurdat_table_DF\")\n",
    "    hurdat_table = spark.sql(hurdat_table_createquery)\n",
    "\n",
    "    print(\"HURDAT schema:\")\n",
    "    hurdat_table.printSchema()\n",
    "\n",
    "    parquet_wr(path_d[\"output_data\"] + \"hurdat_table.parquet\", hurdat_table)\n",
    "    \n",
    "    print(\"HURDAT table complete\")\n",
    "\n",
    "    return hurdat_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table\n",
      "HURDAT schema:\n",
      "root\n",
      " |-- track_id: string (nullable = false)\n",
      " |-- storm_id: string (nullable = false)\n",
      " |-- storm_name: string (nullable = false)\n",
      " |-- sample_count: string (nullable = false)\n",
      " |-- date: string (nullable = false)\n",
      " |-- time: string (nullable = false)\n",
      " |-- system_status: string (nullable = false)\n",
      " |-- latitude: string (nullable = false)\n",
      " |-- longitude: string (nullable = false)\n",
      " |-- max_sust_wind: string (nullable = false)\n",
      " |-- max_pressure: string (nullable = false)\n",
      " |-- NE34: string (nullable = false)\n",
      " |-- SE34: string (nullable = false)\n",
      " |-- SW34: string (nullable = false)\n",
      " |-- NW34: string (nullable = false)\n",
      " |-- NE50: string (nullable = false)\n",
      " |-- SE50: string (nullable = false)\n",
      " |-- SW50: string (nullable = false)\n",
      " |-- NW50: string (nullable = false)\n",
      " |-- NE64: string (nullable = false)\n",
      " |-- SE64: string (nullable = false)\n",
      " |-- SW64: string (nullable = false)\n",
      " |-- NW64: string (nullable = false)\n",
      "\n",
      "HURDAT table DONE\n"
     ]
    }
   ],
   "source": [
    "hurdat_table = process_joined_hurdat_data( spark, path_d, hurdat_storms_df_spark, hurdat_tracks_df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Process Fact table.\\n#immigrations_table_df = process_immigrations_data(spark, PATHS, i94_df_spark_clean, country_codes_i94_df_spark, airport_codes_i94_df_spark, time_table_df, start_str)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Process Fact table.\n",
    "#immigrations_table_df = process_immigrations_data(spark, PATHS, i94_df_spark_clean, country_codes_i94_df_spark, airport_codes_i94_df_spark, time_table_df, start_str)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_data_quality( spark, hurdat_table):\n",
    "    \"\"\"\n",
    "    Check data quality for HURDAT table\n",
    "    \"\"\"\n",
    "\n",
    "    results = { \"hurdat_count\": 0,\n",
    "                \"hurdat\": \"\"           \n",
    "              }\n",
    "\n",
    "    print(\"Checking HURDAT table...\")\n",
    "\n",
    "    hurdat_table.createOrReplaceTempView(\"hurdat_table_DF\")\n",
    "    hurdat_table_check1 = spark.sql(hurdat_table_check1_query)\n",
    "    hurdat_table_flag1 = hurdat_table_check1.collect()[0][0] > 0\n",
    "    hurdat_table_check2 = spark.sql(hurdat_table_check2_query)\n",
    "    hurdat_table_flag2 = hurdat_table_check2.collect()[0][0] < 1\n",
    "\n",
    "    hurdat_flag = any([hurdat_table_flag1, hurdat_table_flag2])\n",
    "\n",
    "    if hurdat_flag:\n",
    "        results['hurdat_count'] = hurdat_table_check2.collect()[0][0]\n",
    "        results['hurdat'] = \"NOK\"\n",
    "    else:\n",
    "        results['hurdat_count'] = hurdat_table_check2.collect()[0][0]\n",
    "        results['hurdat'] = \"OK\" \n",
    "\n",
    "    print(\"NULLS:\")\n",
    "    hurdat_table_check1.show(1)\n",
    "    print(\"ROWS:\")\n",
    "    hurdat_table_check2.show(1)\n",
    "    # --------------------------------------------------------\n",
    "    print(\"Checking data quality complete\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking countries table...\n",
      "NULLS:\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n",
      "ROWS:\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   51840|\n",
      "+--------+\n",
      "\n",
      "Checking data quality DONE\n",
      "[{'hurdat_count': 51840, 'hurdat': 'OK'}]\n"
     ]
    }
   ],
   "source": [
    "results = check_data_quality( spark, hurdat_table)\n",
    "results_all.append(results)\n",
    "\n",
    "print(results_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL pipeline complete\n"
     ]
    }
   ],
   "source": [
    "print(\"ETL pipeline complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
