{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as t\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "from sql_queries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14143744\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def parsell(string):\n",
    "    string = string.strip().lower()\n",
    "\n",
    "    if string.endswith('w') or string.endswith('s'):\n",
    "        sign = -1\n",
    "    else:\n",
    "        sign = 1\n",
    "\n",
    "    string = re.sub(r\"[^0-9.]\", \" \", string).strip()\n",
    "\n",
    "    numeric_ll = float(string)\n",
    "    return numeric_ll * sign\n",
    "\n",
    "def parquet_wr(parquet_filename, df):\n",
    "\n",
    "    start_time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "    \n",
    "    # Write dataframe to parquet file:\n",
    "    df.write.mode(\"overwrite\").parquet(parquet_filename + \"_\" + start_time)\n",
    "\n",
    "    # Read parquet file to Spark dataframe:\n",
    "    df = spark.read.parquet(parquet_filename + \"_\" + start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START ETL pipeline process\n"
     ]
    }
   ],
   "source": [
    "# Start the clocks\n",
    "#start = datetime.now()\n",
    "#start_str = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "print(\"START ETL pipeline process\")\n",
    "results_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def define_paths():\n",
    "    \"\"\"\n",
    "    Define data locations.\n",
    "    \"\"\"\n",
    "    path_d = {}\n",
    "\n",
    "    path_d[\"input_data\"] = 'data/'\n",
    "    path_d[\"output_data\"] = 'data/output_data/'      \n",
    "    path_d[\"data_storage\"] = 'parquet' \n",
    "    path_d[\"hurdat\"] = 'https://www.aoml.noaa.gov/hrd/hurdat/hurdat2.html'\n",
    "        \n",
    "    return path_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "path_d = define_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Create Spark session.\n",
    "    \"\"\"\n",
    "    print(\"Create Spark session\")\n",
    "    \n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    findspark.find()\n",
    "    \n",
    "    conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "    sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "    x = 2\n",
    "    \n",
    "    if x == 1:\n",
    "        spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate() \n",
    "    else:\n",
    "        #spark = SparkSession.builder.getOrCreate()\n",
    "        spark = SparkSession(sc)\n",
    "    \n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Spark session\n"
     ]
    }
   ],
   "source": [
    "# Create Spark session for the pipeline.\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing HURDAT data\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing HURDAT data\")\n",
    "\n",
    "# Read CSV\n",
    "col_names = ['Date','Time','RecordIdentifier','SystemStatus','Latitude','Longitude','MaxSustWind','MaxPressure',\n",
    "             'NE34','SE34','SW34','NW34',\n",
    "             'NE50','SE50','SW50','NW50',\n",
    "             'NE64','SE64','SW64','NW64']\n",
    "hurdat_df = pd.read_csv(path_d[\"hurdat\"], skiprows = 2, low_memory=False, names=col_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial cleaning / reshaping\n",
    "\n",
    "#remove ghost row\n",
    "hurdat_df = hurdat_df.drop([0])\n",
    "\n",
    "#check if row is convoluted header row (contains ALPHA characters)\n",
    "hurdat_df['IsStormHdr'] = ~hurdat_df['Date'].str.isdigit()\n",
    "\n",
    "#create empty columns to receive header data\n",
    "hurdat_df['StormIdentifier'] = ''\n",
    "hurdat_df['StormName'] = ''\n",
    "hurdat_df['StormSamples'] = ''\n",
    "\n",
    "#Iterate over rows to get header data and write to list\n",
    "Lidentifier = []\n",
    "Lname = []\n",
    "Lsamples = []\n",
    "\n",
    "identifier = \"\"\n",
    "name = \"\"\n",
    "samples = \"\"\n",
    "\n",
    "for row in hurdat_df.itertuples(index=True):\n",
    "    if (getattr(row, \"IsStormHdr\") == True):\n",
    "        identifier = getattr(row, \"Date\")\n",
    "        name = getattr(row, \"Time\")\n",
    "        samples = getattr(row, \"RecordIdentifier\")\n",
    "    Lidentifier.append(identifier)\n",
    "    Lname.append(name)\n",
    "    Lsamples.append(samples)    \n",
    "\n",
    "#write list data into dataframe\n",
    "hurdat_df.StormIdentifier = Lidentifier\n",
    "hurdat_df.StormName = Lname\n",
    "hurdat_df.StormSamples = Lsamples    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into storms and tracks\n",
    "hurdat_storms_df = hurdat_df[hurdat_df['IsStormHdr'] == True].copy()\n",
    "hurdat_storms_df = hurdat_storms_df[['StormIdentifier','StormName','StormSamples']]\n",
    "\n",
    "hurdat_tracks_df = hurdat_df[hurdat_df['IsStormHdr'] == False].copy()  \n",
    "\n",
    "\n",
    "Tlatitude = [parsell(lat) for lat in hurdat_tracks_df['Latitude']]\n",
    "hurdat_tracks_df['Latitude'] = Tlatitude\n",
    "\n",
    "Tlongitude = [parsell(lon) for lon in hurdat_tracks_df['Longitude']]\n",
    "hurdat_tracks_df['Longitude'] = Tlongitude \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading HURDAT to Spark\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Read data to Spark\n",
    "print(\"Reading HURDAT to Spark\")\n",
    "hurdat_storm_schema = t.StructType([\n",
    "                            t.StructField(\"StormIdentifier\", t.StringType(), False),\n",
    "                            t.StructField(\"StormName\", t.StringType(), False),\n",
    "                            t.StructField(\"StormSamples\", t.StringType(), False),\n",
    "                        ])\n",
    "\n",
    "hurdat_track_schema = t.StructType([\n",
    "                            t.StructField('Date', t.StringType(), False),\n",
    "                            t.StructField('Time', t.StringType(), False),\n",
    "                            t.StructField('RecordIdentifier', t.StringType(), False),\n",
    "                            t.StructField('SystemStatus', t.StringType(), False),\n",
    "                            t.StructField('Latitude', t.StringType(), False),\n",
    "                            t.StructField('Longitude', t.StringType(), False),\n",
    "                            t.StructField('MaxSustWind', t.StringType(), False),\n",
    "                            t.StructField('MaxPressure', t.StringType(), False),                   \n",
    "                            t.StructField('NE34', t.StringType(), False),\n",
    "                            t.StructField('SE34', t.StringType(), False),\n",
    "                            t.StructField('SW34', t.StringType(), False),\n",
    "                            t.StructField('NW34', t.StringType(), False),    \n",
    "                            t.StructField('NE50', t.StringType(), False),\n",
    "                            t.StructField('SE50', t.StringType(), False),\n",
    "                            t.StructField('SW50', t.StringType(), False),\n",
    "                            t.StructField('NW50', t.StringType(), False),\n",
    "                            t.StructField('NE64', t.StringType(), False),\n",
    "                            t.StructField('SE64', t.StringType(), False),\n",
    "                            t.StructField('SW64', t.StringType(), False),\n",
    "                            t.StructField('NW64', t.StringType(), False),\n",
    "                            t.StructField('IsStormHdr', t.StringType(), False),             \n",
    "                            t.StructField(\"Identifier\", t.StringType(), False),\n",
    "                            t.StructField(\"Name\", t.StringType(), False),\n",
    "                            t.StructField(\"Samples\", t.StringType(), False)\n",
    "                            ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hurdat_storms_df_spark = spark.createDataFrame(hurdat_storms_df, schema=hurdat_storm_schema)\n",
    "hurdat_tracks_df_spark = spark.createDataFrame(hurdat_tracks_df, schema=hurdat_track_schema)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_filename = path_d[\"output_data\"] + \"hurdat_storms_stage.parquet\"\n",
    "df = hurdat_storms_df_spark\n",
    "start_time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dataframe to parquet file:\n",
    "df.write.mode(\"overwrite\").parquet(parquet_filename + \"_\" + start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet file to Spark dataframe:\n",
    "df = spark.read.parquet(parquet_filename + \"_\" + start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HURDAT processing complete\n"
     ]
    }
   ],
   "source": [
    "parquet_wr(path_d[\"output_data\"] + \"hurdat_tracks_stage.parquet\", hurdat_tracks_df_spark)\n",
    "\n",
    "\n",
    "print(\"HURDAT processing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_hurdat_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b4719732f49b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Process all input files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhurdat_df_spark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_hurdat_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mhurdat_storms_df_spark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhurdat_df_spark\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mhurdat_tracks_df_spark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhurdat_df_spark\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'process_hurdat_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Process all input files\n",
    "hurdat_df_spark = process_hurdat_data(spark, path_d)\n",
    "hurdat_storms_df_spark = hurdat_df_spark[0]\n",
    "hurdat_tracks_df_spark = hurdat_df_spark[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Cleaning the data:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Process Dimension tables.\n",
    "\n",
    "def process_joined_hurdat_data( spark, path_d, hurdat_storms_df_spark, hurdat_tracks_df_spark):\n",
    "    \"\"\"\n",
    "    Load input data\n",
    "    Join tables\n",
    "    Write / read the data to / from Spark\n",
    "    Store the data as parquet dimension files\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Creating table\")\n",
    "\n",
    "    hurdat_df_spark_joined = hurdat_storms_df_spark\\\n",
    "                        .join(hurdat_tracks_df_spark, \\\n",
    "                        (hurdat_storms_df_spark.StormIdentifier == hurdat_tracks_df_spark.Identifier))   \n",
    "    \n",
    "    # Create table\n",
    "    hurdat_df_spark_joined.createOrReplaceTempView(\"hurdat_table_DF\")\n",
    "    hurdat_table = spark.sql(hurdat_table_createquery)\n",
    "\n",
    "    print(\"HURDAT schema:\")\n",
    "    hurdat_table.printSchema()\n",
    "\n",
    "    parquet_wr(path_d[\"output_data\"] + \"hurdat_table.parquet\", hurdat_table)\n",
    "    \n",
    "    print(\"HURDAT table complete\")\n",
    "\n",
    "    return hurdat_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "hurdat_table = process_joined_hurdat_data( spark, path_d, hurdat_storms_df_spark, hurdat_tracks_df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Process Fact table.\n",
    "#immigrations_table_df = process_immigrations_data(spark, PATHS, i94_df_spark_clean, country_codes_i94_df_spark, airport_codes_i94_df_spark, time_table_df, start_str)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_data_quality( spark, hurdat_table):\n",
    "    \"\"\"\n",
    "    Check data quality for HURDAT table\n",
    "    \"\"\"\n",
    "\n",
    "    results = { \"hurdat_count\": 0,\n",
    "                \"hurdat\": \"\"           \n",
    "              }\n",
    "\n",
    "    print(\"Checking HURDAT table...\")\n",
    "\n",
    "    hurdat_table.createOrReplaceTempView(\"hurdat_table_DF\")\n",
    "    hurdat_table_check1 = spark.sql(hurdat_table_check1_query)\n",
    "    hurdat_table_flag1 = hurdat_table_check1.collect()[0][0] > 0\n",
    "    hurdat_table_check2 = spark.sql(hurdat_table_check2_query)\n",
    "    hurdat_table_flag2 = hurdat_table_check2.collect()[0][0] < 1\n",
    "\n",
    "    hurdat_flag = any([hurdat_table_flag1, hurdat_table_flag2])\n",
    "\n",
    "    if hurdat_flag:\n",
    "        results['hurdat_count'] = hurdat_table_check2.collect()[0][0]\n",
    "        results['hurdat'] = \"NOK\"\n",
    "    else:\n",
    "        results['hurdat_count'] = hurdat_table_check2.collect()[0][0]\n",
    "        results['hurdat'] = \"OK\" \n",
    "\n",
    "    print(\"NULLS:\")\n",
    "    hurdat_table_check1.show(1)\n",
    "    print(\"ROWS:\")\n",
    "    hurdat_table_check2.show(1)\n",
    "    # --------------------------------------------------------\n",
    "    print(\"Checking data quality complete\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "results = check_data_quality( spark, hurdat_table)\n",
    "results_all.append(results)\n",
    "\n",
    "print(results_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"ETL pipeline complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
