{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as t\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "from sql_queries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14162228\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def parsell(string):\n",
    "    string = string.strip().lower()\n",
    "\n",
    "    if string.endswith('w') or string.endswith('s'):\n",
    "        sign = -1\n",
    "    else:\n",
    "        sign = 1\n",
    "\n",
    "    string = re.sub(r\"[^0-9.]\", \" \", string).strip()\n",
    "\n",
    "    numeric_ll = float(string)\n",
    "    return numeric_ll * sign\n",
    "\n",
    "def parquet_wr(parquet_filename, df):\n",
    "\n",
    "    start_time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "    \n",
    "    # Write dataframe to parquet file:\n",
    "    df.write.mode(\"overwrite\").parquet(parquet_filename + \"_\" + start_time)\n",
    "\n",
    "    # Read parquet file to Spark dataframe:\n",
    "    df = spark.read.parquet(parquet_filename + \"_\" + start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START ETL pipeline process\n"
     ]
    }
   ],
   "source": [
    "# Start the clocks\n",
    "#start = datetime.now()\n",
    "#start_str = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "print(\"START ETL pipeline process\")\n",
    "results_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def define_paths():\n",
    "    \"\"\"\n",
    "    Define data locations.\n",
    "    \"\"\"\n",
    "    path_d = {}\n",
    "\n",
    "    path_d[\"input_data\"] = 'data/'\n",
    "    path_d[\"output_data\"] = 'data/output_data/'      \n",
    "    path_d[\"data_storage\"] = 'parquet' \n",
    "    path_d[\"hurdat\"] = 'https://www.aoml.noaa.gov/hrd/hurdat/hurdat2.html'\n",
    "        \n",
    "    return path_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "path_d = define_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Create Spark session.\n",
    "    \"\"\"\n",
    "    print(\"Create Spark session\")\n",
    "    \n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    findspark.find()\n",
    "    \n",
    "    conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "    sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "    x = 2\n",
    "    \n",
    "    if x == 1:\n",
    "        spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate() \n",
    "    else:\n",
    "        #spark = SparkSession.builder.getOrCreate()\n",
    "        spark = SparkSession(sc)\n",
    "    \n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Spark session\n"
     ]
    }
   ],
   "source": [
    "# Create Spark session for the pipeline.\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef parse_input_files(path_d, path, extension):\\n    # Get (from directory) the files matching extension\\n    all_files = []\\n    for root, dirs, files in os.walk(path):\\n        files = glob.glob(os.path.join(root, extension))\\n        for f in files :\\n            all_files.append(os.path.abspath(f))\\n\\n    return all_files\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def parse_input_files(path_d, path, extension):\n",
    "    # Get (from directory) the files matching extension\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        files = glob.glob(os.path.join(root, extension))\n",
    "        for f in files :\n",
    "            all_files.append(os.path.abspath(f))\n",
    "\n",
    "    return all_files\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_files = parse_input_files(PATHS, PATHS[\"i94_data\"], \"*.sas7bdat\")\\n    input_files_reordered = reorder_paths(input_files)\\n    PATHS[\"i94_files\"] = input_files_reordered\\n    print(f\"i94_files: {PATHS[\\'i94_files\\']}\")\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse input data dir\n",
    "\"\"\"\n",
    "input_files = parse_input_files(PATHS, PATHS[\"i94_data\"], \"*.sas7bdat\")\n",
    "    input_files_reordered = reorder_paths(input_files)\n",
    "    PATHS[\"i94_files\"] = input_files_reordered\n",
    "    print(f\"i94_files: {PATHS['i94_files']}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_hurdat_data(spark, path_d):\n",
    "    \"\"\"\n",
    "    Load input data (HURDAT) from input path\n",
    "    Write / read the data to / from Spark\n",
    "    Store the data as parquet staging files\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Processing HURDAT data\")\n",
    "    \n",
    "    # Read CSV\n",
    "    col_names = ['Date','Time','RecordIdentifier','SystemStatus','Latitude','Longitude','MaxSustWind','MaxPressure',\n",
    "                 'NE34','SE34','SW34','NW34',\n",
    "                 'NE50','SE50','SW50','NW50',\n",
    "                 'NE64','SE64','SW64','NW64']\n",
    "    hurdat_df = pd.read_csv(path_d[\"hurdat\"], skiprows = 2, low_memory=False, names=col_names)\n",
    "    \n",
    "    # Initial cleaning / reshaping\n",
    "    \n",
    "    #remove ghost row\n",
    "    hurdat_df = hurdat_df.drop([0])\n",
    "\n",
    "    #check if row is convoluted header row (contains ALPHA characters)\n",
    "    hurdat_df['IsStormHdr'] = ~hurdat_df['Date'].str.isdigit()\n",
    "\n",
    "    #create empty columns to receive header data\n",
    "    hurdat_df['StormIdentifier'] = ''\n",
    "    hurdat_df['StormName'] = ''\n",
    "    hurdat_df['StormSamples'] = ''\n",
    "    \n",
    "    #Iterate over rows to get header data and write to list\n",
    "    Lidentifier = []\n",
    "    Lname = []\n",
    "    Lsamples = []\n",
    "\n",
    "    identifier = \"\"\n",
    "    name = \"\"\n",
    "    samples = \"\"\n",
    "\n",
    "    for row in hurdat_df.itertuples(index=True):\n",
    "        if (getattr(row, \"IsStormHdr\") == True):\n",
    "            identifier = getattr(row, \"Date\")\n",
    "            name = getattr(row, \"Time\")\n",
    "            samples = getattr(row, \"RecordIdentifier\")\n",
    "        Lidentifier.append(identifier)\n",
    "        Lname.append(name)\n",
    "        Lsamples.append(samples)    \n",
    "    \n",
    "    #write list data into dataframe\n",
    "    hurdat_df.StormIdentifier = Lidentifier\n",
    "    hurdat_df.StormName = Lname\n",
    "    hurdat_df.StormSamples = Lsamples    \n",
    "    \n",
    "    #split into storms and tracks\n",
    "    hurdat_storms_df = hurdat_df[hurdat_df['IsStormHdr'] == True].copy()\n",
    "    hurdat_storms_df = hurdat_storms_df[['StormIdentifier','StormName','StormSamples']]\n",
    "\n",
    "    hurdat_tracks_df = hurdat_df[hurdat_df['IsStormHdr'] == False].copy()  \n",
    "    \n",
    "    \n",
    "    Tlatitude = [parsell(lat) for lat in hurdat_tracks_df['Latitude']]\n",
    "    hurdat_tracks_df['Latitude'] = Tlatitude\n",
    "\n",
    "    Tlongitude = [parsell(lon) for lon in hurdat_tracks_df['Longitude']]\n",
    "    hurdat_tracks_df['Longitude'] = Tlongitude \n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # Read data to Spark\n",
    "    print(\"Reading HURDAT to Spark\")\n",
    "    hurdat_storm_schema = t.StructType([\n",
    "                                t.StructField(\"StormIdentifier\", t.StringType(), False),\n",
    "                                t.StructField(\"StormName\", t.StringType(), False),\n",
    "                                t.StructField(\"StormSamples\", t.StringType(), False),\n",
    "                            ])\n",
    "\n",
    "    hurdat_track_schema = t.StructType([\n",
    "                                t.StructField('Date', t.StringType(), False),\n",
    "                                t.StructField('Time', t.StringType(), False),\n",
    "                                t.StructField('RecordIdentifier', t.StringType(), False),\n",
    "                                t.StructField('SystemStatus', t.StringType(), False),\n",
    "                                t.StructField('Latitude', t.StringType(), False),\n",
    "                                t.StructField('Longitude', t.StringType(), False),\n",
    "                                t.StructField('MaxSustWind', t.StringType(), False),\n",
    "                                t.StructField('MaxPressure', t.StringType(), False),                   \n",
    "                                t.StructField('NE34', t.StringType(), False),\n",
    "                                t.StructField('SE34', t.StringType(), False),\n",
    "                                t.StructField('SW34', t.StringType(), False),\n",
    "                                t.StructField('NW34', t.StringType(), False),    \n",
    "                                t.StructField('NE50', t.StringType(), False),\n",
    "                                t.StructField('SE50', t.StringType(), False),\n",
    "                                t.StructField('SW50', t.StringType(), False),\n",
    "                                t.StructField('NW50', t.StringType(), False),\n",
    "                                t.StructField('NE64', t.StringType(), False),\n",
    "                                t.StructField('SE64', t.StringType(), False),\n",
    "                                t.StructField('SW64', t.StringType(), False),\n",
    "                                t.StructField('NW64', t.StringType(), False),\n",
    "                                t.StructField('IsStormHdr', t.StringType(), False),             \n",
    "                                t.StructField(\"Identifier\", t.StringType(), False),\n",
    "                                t.StructField(\"Name\", t.StringType(), False),\n",
    "                                t.StructField(\"Samples\", t.StringType(), False)\n",
    "                                ])\n",
    "\n",
    "    hurdat_storms_df_spark = spark.createDataFrame(hurdat_storms_df, schema=hurdat_storm_schema)\n",
    "    hurdat_tracks_df_spark = spark.createDataFrame(hurdat_tracks_df, schema=hurdat_track_schema)\n",
    "\n",
    "    \n",
    "    parquet_wr(path_d[\"output_data\"] + \"hurdat_storms_stage.parquet\", hurdat_storms_df_spark)\n",
    "    parquet_wr(path_d[\"output_data\"] + \"hurdat_tracks_stage.parquet\", hurdat_tracks_df_spark)\n",
    "\n",
    "    \n",
    "    print(\"HURDAT processing complete\")\n",
    "\n",
    "    return hurdat_storms_df_spark, hurdat_tracks_df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing HURDAT data\n",
      "Reading HURDAT to Spark\n",
      "HURDAT processing complete\n"
     ]
    }
   ],
   "source": [
    "# Process all input files\n",
    "hurdat_df_spark = process_hurdat_data(spark, path_d)\n",
    "hurdat_storms_df_spark = hurdat_df_spark[0]\n",
    "hurdat_tracks_df_spark = hurdat_df_spark[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Cleaning the data:\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Cleaning the data:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Process Dimension tables.\n",
    "\n",
    "def process_joined_hurdat_data( spark, path_d, hurdat_storms_df_spark, hurdat_tracks_df_spark):\n",
    "    \"\"\"\n",
    "    Load input data\n",
    "    Join tables\n",
    "    Write / read the data to / from Spark\n",
    "    Store the data as parquet dimension files\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Creating table\")\n",
    "\n",
    "    hurdat_df_spark_joined = hurdat_storms_df_spark\\\n",
    "                        .join(hurdat_tracks_df_spark, \\\n",
    "                        (hurdat_storms_df_spark.StormIdentifier == hurdat_tracks_df_spark.Identifier))   \n",
    "    \n",
    "    # Create table\n",
    "    hurdat_df_spark_joined.createOrReplaceTempView(\"hurdat_table_DF\")\n",
    "    hurdat_table = spark.sql(hurdat_table_createquery)\n",
    "\n",
    "    print(\"HURDAT schema:\")\n",
    "    hurdat_table.printSchema()\n",
    "\n",
    "    parquet_wr(path_d[\"output_data\"] + \"hurdat_table.parquet\", hurdat_table)\n",
    "    \n",
    "    print(\"HURDAT table complete\")\n",
    "\n",
    "    return hurdat_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table\n",
      "HURDAT schema:\n",
      "root\n",
      " |-- track_id: string (nullable = false)\n",
      " |-- storm_id: string (nullable = false)\n",
      " |-- storm_name: string (nullable = false)\n",
      " |-- sample_count: string (nullable = false)\n",
      " |-- date: string (nullable = false)\n",
      " |-- time: string (nullable = false)\n",
      " |-- system_status: string (nullable = false)\n",
      " |-- latitude: string (nullable = false)\n",
      " |-- longitude: string (nullable = false)\n",
      " |-- max_sust_wind: string (nullable = false)\n",
      " |-- max_pressure: string (nullable = false)\n",
      " |-- NE34: string (nullable = false)\n",
      " |-- SE34: string (nullable = false)\n",
      " |-- SW34: string (nullable = false)\n",
      " |-- NW34: string (nullable = false)\n",
      " |-- NE50: string (nullable = false)\n",
      " |-- SE50: string (nullable = false)\n",
      " |-- SW50: string (nullable = false)\n",
      " |-- NW50: string (nullable = false)\n",
      " |-- NE64: string (nullable = false)\n",
      " |-- SE64: string (nullable = false)\n",
      " |-- SW64: string (nullable = false)\n",
      " |-- NW64: string (nullable = false)\n",
      "\n",
      "HURDAT table complete\n"
     ]
    }
   ],
   "source": [
    "hurdat_table = process_joined_hurdat_data( spark, path_d, hurdat_storms_df_spark, hurdat_tracks_df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Process Fact table.\\n#immigrations_table_df = process_immigrations_data(spark, PATHS, i94_df_spark_clean, country_codes_i94_df_spark, airport_codes_i94_df_spark, time_table_df, start_str)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Process Fact table.\n",
    "#immigrations_table_df = process_immigrations_data(spark, PATHS, i94_df_spark_clean, country_codes_i94_df_spark, airport_codes_i94_df_spark, time_table_df, start_str)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_data_quality( spark, hurdat_table):\n",
    "    \"\"\"\n",
    "    Check data quality for HURDAT table\n",
    "    \"\"\"\n",
    "\n",
    "    results = { \"hurdat_count\": 0,\n",
    "                \"hurdat\": \"\"           \n",
    "              }\n",
    "\n",
    "    print(\"Checking HURDAT table...\")\n",
    "\n",
    "    hurdat_table.createOrReplaceTempView(\"hurdat_table_DF\")\n",
    "    hurdat_table_check1 = spark.sql(hurdat_table_check1_query)\n",
    "    hurdat_table_flag1 = hurdat_table_check1.collect()[0][0] > 0\n",
    "    hurdat_table_check2 = spark.sql(hurdat_table_check2_query)\n",
    "    hurdat_table_flag2 = hurdat_table_check2.collect()[0][0] < 1\n",
    "\n",
    "    hurdat_flag = any([hurdat_table_flag1, hurdat_table_flag2])\n",
    "\n",
    "    if hurdat_flag:\n",
    "        results['hurdat_count'] = hurdat_table_check2.collect()[0][0]\n",
    "        results['hurdat'] = \"NOK\"\n",
    "    else:\n",
    "        results['hurdat_count'] = hurdat_table_check2.collect()[0][0]\n",
    "        results['hurdat'] = \"OK\" \n",
    "\n",
    "    print(\"NULLS:\")\n",
    "    hurdat_table_check1.show(1)\n",
    "    print(\"ROWS:\")\n",
    "    hurdat_table_check2.show(1)\n",
    "    # --------------------------------------------------------\n",
    "    print(\"Checking data quality complete\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking HURDAT table...\n",
      "NULLS:\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n",
      "ROWS:\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   51840|\n",
      "+--------+\n",
      "\n",
      "Checking data quality complete\n",
      "[{'hurdat_count': 51840, 'hurdat': 'OK'}]\n"
     ]
    }
   ],
   "source": [
    "results = check_data_quality( spark, hurdat_table)\n",
    "results_all.append(results)\n",
    "\n",
    "print(results_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL pipeline complete\n"
     ]
    }
   ],
   "source": [
    "print(\"ETL pipeline complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
